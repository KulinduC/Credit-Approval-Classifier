import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, SplineTransformer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import make_pipeline
from sklearn.metrics import accuracy_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.linear_model import LogisticRegressionCV
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.cross_decomposition import PLSRegression
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA

# Load & name the columns
cols = ["Gender","Age","Debt","MaritalStatus","BankCustomer","EducationLevel",
        "Ethnicity","YearsEmployed","PriorDefault","Employed","CreditScore",
        "DriversLicense","Citizen","ZipCode","Income","Approved"]

df = pd.read_csv("credit+approval/crx.data", header=None, names=cols, na_values=["?"])
df = df.dropna().copy()

# Binary encodings (same mapping as R)
df["Gender"]         = (df["Gender"] == "a").astype(int)
df["PriorDefault"]   = (df["PriorDefault"] == "t").astype(int)
df["Employed"]       = (df["Employed"] == "t").astype(int)
df["DriversLicense"] = (df["DriversLicense"] == "t").astype(int)
df["Approved"]       = (df["Approved"] == "+").astype(int)

# Ensure numeric columns
num_cols = ["Age","Debt","YearsEmployed","CreditScore","Income","ZipCode"]
for c in num_cols:
    df[c] = pd.to_numeric(df[c], errors="coerce")
df = df.dropna().copy()

# Zip buckets then drop ZipCode
df["ZipCode1"] = (df["ZipCode"] <= 73).astype(int)
df["ZipCode2"] = ((df["ZipCode"] > 73) & (df["ZipCode"] <= 160)).astype(int)
df["ZipCode3"] = ((df["ZipCode"] > 160) & (df["ZipCode"] <= 272)).astype(int)
df["ZipCode4"] = (df["ZipCode"] > 272).astype(int)
df = df.drop(columns=["ZipCode"])

# Citizen g/p
df["Citizen_g"] = (df["Citizen"] == "g").astype(int)
df["Citizen_p"] = (df["Citizen"] == "p").astype(int)
df = df.drop(columns=["Citizen"])

# MaritalStatus u/y/l
df["MaritalStatus_u"] = (df["MaritalStatus"] == "u").astype(int)
df["MaritalStatus_y"] = (df["MaritalStatus"] == "y").astype(int)
df["MaritalStatus_l"] = (df["MaritalStatus"] == "l").astype(int)
df = df.drop(columns=["MaritalStatus"])

# BankCustomer g/p
df["BankCustomer_g"] = (df["BankCustomer"] == "g").astype(int)
df["BankCustomer_p"] = (df["BankCustomer"] == "p").astype(int)
df = df.drop(columns=["BankCustomer"])

# EducationLevel
for lv in ["c","d","cc","i","j","k","m","r","q","w","x","e","aa"]:
    df[f"EducationLevel_{lv}"] = (df["EducationLevel"] == lv).astype(int)
df = df.drop(columns=["EducationLevel"])

# Ethnicity
for lv in ["v","h","bb","j","n","z","dd","ff"]:
    df[f"Ethnicity_{lv}"] = (df["Ethnicity"] == lv).astype(int)
df = df.drop(columns=["Ethnicity"])

# Train/test split (80/20)
data = df.drop(columns=["Approved"])
target = df["Approved"].astype(int)

train_features, test_features, train_labels, test_labels = train_test_split(
    data, target, test_size=0.2, stratify=target, random_state=1
)

# Scale ONLY the 5 continuous numeric columns (train stats)
numCols = ["Age","Debt","YearsEmployed","CreditScore","Income"]

scaler = StandardScaler().fit(train_features[numCols])
train_features_scaled = train_features.copy()
test_features_scaled  = test_features.copy()

train_features_scaled[numCols] = scaler.transform(train_features[numCols])
test_features_scaled[numCols]  = scaler.transform(test_features[numCols])


# Significant feature subset (same as R)
sig_vars = ["CreditScore","YearsEmployed","Income","Debt","Age","PriorDefault","Employed","DriversLicense"]

train_sig = train_features_scaled[sig_vars].to_numpy()
test_sig = test_features_scaled[sig_vars].to_numpy()

train_all = train_features_scaled.to_numpy()
test_all = test_features_scaled.to_numpy()

# Helper functions
def Accuracy(label, acc):
    print(f"{label:<32s} : {acc:6.2f}%")

def AccLabels(model, X, y):
    return accuracy_score(y, model.predict(X)) * 100

def AccScores(scores, y):  # for regression-style outputs
    return accuracy_score(y, (scores > 0.5).astype(int)) * 100


# Baseline (majority class)
base = test_labels.value_counts(normalize=True).max() * 100
print(f"Baseline (majority-class)       : {base:6.2f}%\n")


# MODELS

# Random Forest (mtry ~ 3 in R -> max_features=3)
rf_sig = RandomForestClassifier(n_estimators=500, max_features=3, random_state=1).fit(train_sig, train_labels)
rf_all = RandomForestClassifier(n_estimators=500, max_features=3, random_state=1).fit(train_all, train_labels)

# Linear Regression (then 0.5 threshold)
lm_sig = LinearRegression().fit(train_sig, train_labels)
lm_all = LinearRegression().fit(train_all, train_labels)

# Logistic Regression (plain)
log_sig = LogisticRegression(solver="liblinear", max_iter=1000).fit(train_sig, train_labels)
log_all = LogisticRegression(solver="liblinear", max_iter=1000).fit(train_all, train_labels)

# SVMs
svm_lin_sig = SVC(kernel="linear", C=1).fit(train_sig, train_labels)
svm_lin_all = SVC(kernel="linear", C=1).fit(train_all, train_labels)

svm_rbf_sig = SVC(kernel="rbf", C=1, gamma="scale").fit(train_sig, train_labels)
svm_rbf_all = SVC(kernel="rbf", C=1, gamma="scale").fit(train_all, train_labels)

# KNN
knn_sig = KNeighborsClassifier(n_neighbors=5).fit(train_sig, train_labels)
knn_all = KNeighborsClassifier(n_neighbors=5).fit(train_all, train_labels)

# PLS (regression, then 0.5 threshold)
ncomp_sig = min(5, train_sig.shape[1])
ncomp_all = min(5, train_all.shape[1])
pls_sig = PLSRegression(n_components=ncomp_sig).fit(train_sig, train_labels)
pls_all = PLSRegression(n_components=ncomp_all).fit(train_all, train_labels)

# LDA / QDA on non-zero significant numerics
nonZero_sig = ["CreditScore","YearsEmployed","Income","Debt","Age"]
train_nz = train_features_scaled[nonZero_sig].to_numpy()
test_nz = test_features_scaled[nonZero_sig].to_numpy()
lda_sig = LDA().fit(train_nz, train_labels)
qda_sig = QDA(reg_param=1e-4).fit(train_nz, train_labels)  # small regularization for stability

# Splines (natural-ish cubic via SplineTransformer + Logistic)
# Significant: splines on the 5 numerics, passthrough binary PriorDefault/Employed/DriversLicense
num_sig = ["CreditScore","YearsEmployed","Income","Debt","Age"]
bin_sig = ["PriorDefault","Employed","DriversLicense"]  # just pass these through

ct_sig = ColumnTransformer(
    transformers=[
        ("spline", SplineTransformer(degree=3, n_knots=5, extrapolation="linear", include_bias=False), num_sig),
        ("passthrough", "passthrough", bin_sig),
    ],
    remainder="drop"
)
spl_log_sig = make_pipeline(ct_sig, LogisticRegression(solver="liblinear", max_iter=1000)).fit(
    train_features_scaled[sig_vars], train_labels
)

# All: splines on 5 numerics,
all_other_cols = [c for c in train_features_scaled.columns if c not in numCols]
ct_all = ColumnTransformer(
    transformers=[
        ("spline", SplineTransformer(degree=3, n_knots=5, extrapolation="linear", include_bias=False), numCols),
        ("passthrough", "passthrough", [c for c in all_other_cols if c != "Approved"]),
    ],
    remainder="drop"
)
# Fit on full train_features_scaled (all features)
spl_log_all = make_pipeline(ct_all, LogisticRegression(solver="liblinear", max_iter=1000)).fit(
    train_features_scaled, train_labels
)

# Ridge / Lasso (logistic);
ridge_sig = LogisticRegression(penalty="l2", solver="liblinear", max_iter=1000).fit(train_sig, train_labels)
ridge_all = LogisticRegression(penalty="l2", solver="liblinear", max_iter=1000).fit(train_all, train_labels)

lasso_sig = LogisticRegression(penalty="l1", solver="saga", max_iter=5000).fit(train_sig, train_labels)
lasso_all = LogisticRegression(penalty="l1", solver="saga", max_iter=5000).fit(train_all, train_labels)

# PRINTOUTS
print("\n=== MODEL ACCURACIES ===")
# Linear / Logistic / Splines
Accuracy("Linear Reg (significant)",  AccScores(lm_sig.predict(test_sig), test_labels))
Accuracy("Linear Reg (all)",          AccScores(lm_all.predict(test_all), test_labels))
Accuracy("Logistic Reg (significant)",AccLabels(log_sig, test_sig, test_labels))
Accuracy("Logistic Reg (all)",        AccLabels(log_all, test_all, test_labels))
Accuracy("Splines (significant)",     AccLabels(spl_log_sig, test_features_scaled[sig_vars], test_labels))
Accuracy("Splines (all)",             AccLabels(spl_log_all, test_features_scaled, test_labels))

# Random Forest
Accuracy("Random Forest (significant)",AccLabels(rf_sig, test_sig, test_labels))
Accuracy("Random Forest (all)",        AccLabels(rf_all, test_all, test_labels))

# SVM
Accuracy("SVM Linear (significant)",   AccLabels(svm_lin_sig, test_sig, test_labels))
Accuracy("SVM Linear (all)",           AccLabels(svm_lin_all, test_all, test_labels))
Accuracy("SVM RBF (significant)",      AccLabels(svm_rbf_sig, test_sig, test_labels))
Accuracy("SVM RBF (all)",              AccLabels(svm_rbf_all, test_all, test_labels))

# KNN
Accuracy("KNN (significant)",          AccLabels(knn_sig, test_sig, test_labels))
Accuracy("KNN (all)",                  AccLabels(knn_all, test_all, test_labels))

# PLS (thresholded)
Accuracy("PLS (significant)",          AccScores(pls_sig.predict(test_sig).ravel(), test_labels))
Accuracy("PLS (all)",                  AccScores(pls_all.predict(test_all).ravel(), test_labels))

# LDA / QDA (nonzero significant numerics)
Accuracy("LDA (significant-nonzero)",  AccLabels(lda_sig, test_nz, test_labels))
Accuracy("QDA (significant-nonzero)",  AccLabels(qda_sig, test_nz, test_labels))

# Ridge / Lasso (logistic)
Accuracy("Ridge (significant)",        AccLabels(ridge_sig, test_sig, test_labels))
Accuracy("Ridge (all)",                AccLabels(ridge_all, test_all, test_labels))
Accuracy("Lasso (significant)",        AccLabels(lasso_sig, test_sig, test_labels))
Accuracy("Lasso (all)",                AccLabels(lasso_all, test_all, test_labels))